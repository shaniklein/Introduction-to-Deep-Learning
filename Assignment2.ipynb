{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaniklein/Introduction-to-Deep-Learning/blob/main/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx3i2Op-6X5n"
      },
      "source": [
        "# Assignment 2: Word Prediction\n",
        "\n",
        "**Deadline**: Sunday, April 18th, by 9pm.\n",
        "\n",
        "**Submission**: Submit a PDF export of the completed notebook as well as the ipynb file. \n",
        "\n",
        " \n",
        "\n",
        "In this assignment, we will make a neural network that can predict the next word\n",
        "in a sentence given the previous three.  \n",
        "In doing this prediction task, our neural networks will learn about *words* and about\n",
        "how to represent words. We'll explore the *vector representations* of words that our\n",
        "model produces, and analyze these representations.\n",
        "\n",
        "You may modify the starter code as you see fit, including changing the signatures of functions and adding/removing helper functions. However, please make sure that you properly explain what you are doing and why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zdEvcdO6X5s"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "from collections import Counter\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQYtUQTH6X5t"
      },
      "source": [
        "## Question 1. Data (15%)\n",
        "\n",
        "With any machine learning problem, the first thing that we would want to do\n",
        "is to get an intuitive understanding of what our data looks like. Download the file\n",
        "`raw_sentences.txt` from the course page on Moodle and upload it to Google Drive.\n",
        "Then, mount Google Drive from your Google Colab notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eWXHhCe6X5t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d38061a-bc0f-4b2b-c128-31b04a39bafe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hteg6bwv6X5t"
      },
      "source": [
        "Find the path to `raw_sentences.txt`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALMsGfFi6X5u"
      },
      "source": [
        "file_path = '/content/gdrive/My Drive/Intro_to_Deep_Learning/assignment2/raw_sentences.txt' # TODO - UPDATE ME!"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD5uXTle6X5u"
      },
      "source": [
        "The following code reads the sentences in our file, split each sentence into\n",
        "its individual words, and stores the sentences (list of words) in the\n",
        "variable `sentences`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75NXJO_T6X5v"
      },
      "source": [
        "sentences = []\n",
        "for line in open(file_path):\n",
        "    words = line.split()\n",
        "    sentence = [word.lower() for word in words]\n",
        "    sentences.append(sentence)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbP0-e_U6X5v"
      },
      "source": [
        "There are 97,162 sentences in total, and \n",
        "these sentences are composed of 250 distinct words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLUp8rZT6X5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1287ec9-c83d-4c3d-89e7-97c39a8a56a5"
      },
      "source": [
        "vocab = set([w for s in sentences for w in s])\n",
        "print(len(sentences)) # 97162\n",
        "print(len(vocab)) # 250"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97162\n",
            "250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB77tJrU6X5v"
      },
      "source": [
        "We'll separate our data into training, validation, and test.\n",
        "We'll use `10,000 sentences for test, 10,000 for validation, and\n",
        "the rest for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJQRB6RJ6X5v"
      },
      "source": [
        "test, valid, train = sentences[:10000], sentences[10000:20000], sentences[20000:]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUj7fsi06X5v"
      },
      "source": [
        "### Part (a) -- 2%\n",
        "\n",
        "**Display** 10 sentences in the training set.\n",
        "**Explain** how punctuations are treated in our word representation, and how words\n",
        "with apostrophes are represented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90AmLcpF6X5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbda348a-1ba2-45cd-ed1e-e7bcaf707341"
      },
      "source": [
        "for i,sent in enumerate(train[:50]):\n",
        "  print(str(i+1)+\".\", \" \".join(sent))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. last night , he said , did it for me .\n",
            "2. on what can i do ?\n",
            "3. now where does it go ?\n",
            "4. what did the court do ?\n",
            "5. but at the same time , we have a long way to go .\n",
            "6. that was the only way .\n",
            "7. this team will be back .\n",
            "8. so that is what i do .\n",
            "9. we have a right to know .\n",
            "10. now they are three .\n",
            "11. but for me , now , this is it .\n",
            "12. she 's still there for us .\n",
            "13. it 's part of this game , man .\n",
            "14. it was : how do we get there ?\n",
            "15. but they do nt last too long .\n",
            "16. more are like me , she said .\n",
            "17. who do you think they want to be like ?\n",
            "18. no , he could not .\n",
            "19. so i left it up to them .\n",
            "20. we were nt right .\n",
            "21. we are a good team .\n",
            "22. i did nt know about it , she said .\n",
            "23. i do nt think we are going to make that .\n",
            "24. we did nt want to make one of those .\n",
            "25. that 's what 's good about it .\n",
            "26. now there are so many .\n",
            "27. then there 's the music .\n",
            "28. who is the we ?\n",
            "29. but i want it to be the same , i said .\n",
            "30. we never left the house .\n",
            "31. this is the people 's team .\n",
            "32. and what he does not want to do .\n",
            "33. every place is new york today .\n",
            "34. people want to do it all , he says .\n",
            "35. it is out there .\n",
            "36. for a school night ?\n",
            "37. that , i know i can do .\n",
            "38. it was good for all of us .\n",
            "39. but they are not going to do me in .\n",
            "40. and in every case , they were .\n",
            "41. what does it do for you in this world ?\n",
            "42. i was in the way , he said .\n",
            "43. they think of what if ?\n",
            "44. we might as well be home today , he said .\n",
            "45. i did nt see what was going on .\n",
            "46. all of them are the same .\n",
            "47. he did nt much like it .\n",
            "48. you know more than you think you do .\n",
            "49. she did nt want any and he did .\n",
            "50. does nt work that way .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swyMJHYN-Taa"
      },
      "source": [
        "**Write your answers here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2erKpOJ6X5w"
      },
      "source": [
        "\n",
        "### Part (b) -- 3%\n",
        "\n",
        "**Print** the 10 most common words in the vocabulary and how often does each of these\n",
        "words appear in the training sentences. Express the second quantity as a percentage\n",
        "(i.e. number of occurences of the  word / total number of words in the training set).\n",
        "\n",
        "These are useful quantities to compute, because one of the first things a machine learning model will learn is to predict the **most common** class. Getting a sense of the\n",
        "distribution of our data will help you understand our model's behaviour.\n",
        "\n",
        "You can use Python's `collections.Counter` class if you would like to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqSZO_a36X5w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "a480149e-f809-47b2-a260-4d6799eca660"
      },
      "source": [
        "train_words=[]\n",
        "for sentence in train:\n",
        "  for word in sentence:\n",
        "    train_words.append(word)\n",
        "\n",
        "c=Counter(train_words)\n",
        "freq=c.most_common(10)\n",
        "df=pd.DataFrame(freq,columns=['word','num of occ'])\n",
        "df['precentege']=(df['num of occ']/len(train_words))*100\n",
        "df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>num of occ</th>\n",
              "      <th>precentege</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>.</td>\n",
              "      <td>64297</td>\n",
              "      <td>10.695720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it</td>\n",
              "      <td>23118</td>\n",
              "      <td>3.845648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>,</td>\n",
              "      <td>19537</td>\n",
              "      <td>3.249954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i</td>\n",
              "      <td>17684</td>\n",
              "      <td>2.941710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>do</td>\n",
              "      <td>16181</td>\n",
              "      <td>2.691688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>to</td>\n",
              "      <td>15490</td>\n",
              "      <td>2.576741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>nt</td>\n",
              "      <td>13009</td>\n",
              "      <td>2.164030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>?</td>\n",
              "      <td>12881</td>\n",
              "      <td>2.142737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>the</td>\n",
              "      <td>12583</td>\n",
              "      <td>2.093165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>'s</td>\n",
              "      <td>12552</td>\n",
              "      <td>2.088008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  word  num of occ  precentege\n",
              "0    .       64297   10.695720\n",
              "1   it       23118    3.845648\n",
              "2    ,       19537    3.249954\n",
              "3    i       17684    2.941710\n",
              "4   do       16181    2.691688\n",
              "5   to       15490    2.576741\n",
              "6   nt       13009    2.164030\n",
              "7    ?       12881    2.142737\n",
              "8  the       12583    2.093165\n",
              "9   's       12552    2.088008"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4CHlVRI6X5w"
      },
      "source": [
        "### Part (c) -- 10%\n",
        "\n",
        "Our neural network will take as input three words and predict the next one. Therefore, we need our data set to be comprised of seuqnces of four consecutive words in a sentence, referred to as *4grams*. \n",
        "\n",
        "**Complete** the helper functions `convert_words_to_indices` and\n",
        "`generate_4grams`, so that the function `process_data` will take a \n",
        "list of sentences (i.e. list of list of words), and generate an \n",
        "$N \\times 4$ numpy matrix containing indices of 4 words that appear\n",
        "next to each other, where $N$ is the number of 4grams (sequences of 4 words appearing one after the other) that can be found in the complete list of sentences. Examples of how these functions should operate are detailed in the code below. \n",
        "\n",
        "You can use the defined `vocab`, `vocab_itos`,\n",
        "and `vocab_stoi` in your code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPQ9bZR4safN"
      },
      "source": [
        "# A list of all the words in the data set. We will assign a unique \n",
        "# identifier for each of these words.\n",
        "vocab = sorted(list(set([w for s in train for w in s])))\n",
        "# A mapping of index => word (string)\n",
        "vocab_itos = dict(enumerate(vocab))\n",
        "# A mapping of word => its index\n",
        "vocab_stoi = {word:index for index, word in vocab_itos.items()}\n",
        "\n",
        "def convert_words_to_indices(sents):\n",
        "    \"\"\"\n",
        "    This function takes a list of sentences (list of list of words)\n",
        "    and returns a new list with the same structure, but where each word\n",
        "    is replaced by its index in `vocab_stoi`.\n",
        "\n",
        "    Example:\n",
        "    >>> convert_words_to_indices([['one', 'in', 'five', 'are', 'over', 'here'], ['other', 'one', 'since', 'yesterday'], ['you']])\n",
        "    [[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]]\n",
        "    \"\"\"\n",
        "    new_sents=[]\n",
        "    for s in sents:\n",
        "      new_sents.append([vocab_stoi[w] for w in s])\n",
        "    return new_sents\n",
        "\n",
        "def generate_4grams(seqs):\n",
        "    \"\"\"\n",
        "    This function takes a list of sentences (list of lists) and returns\n",
        "    a new list containing the 4-grams (four consequentively occuring words)\n",
        "    that appear in the sentences. Note that a unique 4-gram can appear multiple\n",
        "    times, one per each time that the 4-gram appears in the data parameter `seqs`.\n",
        "\n",
        "    Example:\n",
        "\n",
        "    >>> generate_4grams([[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]])\n",
        "    [[148, 98, 70, 23], [98, 70, 23, 154], [70, 23, 154, 89], [151, 148, 181, 246]]\n",
        "    >>> generate_4grams([[1, 1, 1, 1, 1]])\n",
        "    [[1, 1, 1, 1], [1, 1, 1, 1]]\n",
        "    \"\"\"\n",
        "    four_grams=[]\n",
        "    for seq in seqs:\n",
        "      if (len(seq)<4):\n",
        "        continue\n",
        "      \n",
        "      start=0\n",
        "      end=len(seq)\n",
        "\n",
        "      while(start+4<=end):\n",
        "        four_grams.append(seq[start:start+4])\n",
        "        start+=1\n",
        "    return four_grams\n",
        "\n",
        "def process_data(sents):\n",
        "    \"\"\"\n",
        "    This function takes a list of sentences (list of lists), and generates an\n",
        "    numpy matrix with shape [N, 4] containing indices of words in 4-grams.\n",
        "    \"\"\"\n",
        "    indices = convert_words_to_indices(sents)\n",
        "    fourgrams = generate_4grams(indices)\n",
        "    return np.array(fourgrams)\n",
        "\n",
        "# We can now generate our data which will be used to train and test the network\n",
        "train4grams = process_data(train)\n",
        "valid4grams = process_data(valid)\n",
        "test4grams = process_data(test)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOWlJQIHt9k8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39648e2d-fc64-4d4a-8ca9-d14d0329b6bf"
      },
      "source": [
        "convert_words_to_indices([['one', 'in', 'five', 'are', 'over', 'here'], ['other', 'one', 'since', 'yesterday'], ['you']])\n",
        "# assert : [[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUZsxdHk6X5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8792463-a8d5-4e41-81dc-cacbc4dd4a55"
      },
      "source": [
        "generate_4grams([[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]])\n",
        "#   [[148, 98, 70, 23], [98, 70, 23, 154], [70, 23, 154, 89], [151, 148, 181, 246]]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[148, 98, 70, 23], [98, 70, 23, 154], [70, 23, 154, 89], [151, 148, 181, 246]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1EVn7YcwTKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bfec449-4111-4420-f759-72d037cad6d6"
      },
      "source": [
        " generate_4grams([[1, 1, 1, 1, 1]])\n",
        "  #  [[1, 1, 1, 1], [1, 1, 1, 1]]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1], [1, 1, 1, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rv-6sNm6X5x"
      },
      "source": [
        "## Question 2. A Multi-Layer Perceptron (40%)\n",
        "\n",
        "In this section, we will build a two-layer multi-layer perceptron. \n",
        "Our model will look like this:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=15uMLB-YsMHSOr0EQfTRhWd4o9enIOjUe\">\n",
        "\n",
        "Since the sentences in the data are comprised of $250$ distinct words, our task boils down to claissfication where the label space $\\mathcal{S}$ is of cardinality $|\\mathcal{S}|=250$ while our input, which is comprised of a combination of three words, is treated as a vector of size $750\\times 1$ (i.e., the concatanation of three one-hot $250\\times 1$ vectors).\n",
        "\n",
        "The following function `get_batch` will take as input the whole dataset and output a single batch for the training. The output size of the batch is explained below.\n",
        "\n",
        "**Implement** yourself a function `make_onehot` which takes the data in index notation and output it in a onehot notation.\n",
        "\n",
        "Start by reviewing the helper function, which is given to you:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsqTLOoJ6X5x"
      },
      "source": [
        "def make_onehot(data):\n",
        "    \n",
        "    \"\"\"\n",
        "    Convert one batch of data in the index notation into its corresponding onehot\n",
        "    notation. Remember, the function should work for both xt and st. \n",
        "    input - vector with shape D (1D or 2D)\n",
        "    output - vector with shape (D,250)\n",
        "    \"\"\"\n",
        "\n",
        "    output=[]\n",
        "    \n",
        "    for line in data:\n",
        "      if data.ndim==1:\n",
        "        line=[line]\n",
        "\n",
        "      one_hot_line=np.zeros((len(line),250))\n",
        "      for i,d in enumerate(line):\n",
        "        one_hot_line[i][d]=1\n",
        "      output.append(one_hot_line)\n",
        "\n",
        "    return np.array(output)\n",
        "        \n",
        "     \n",
        "\n",
        "def get_batch(data, range_min, range_max, onehot=True):\n",
        "    \"\"\"\n",
        "    Convert one batch of data in the form of 4-grams into input and output\n",
        "    data and return the training data (xt, st) where:\n",
        "     - `xt` is an numpy array of one-hot vectors of shape [batch_size, 3, 250]\n",
        "     - `st` is either\n",
        "            - a numpy array of shape [batch_size, 250] if onehot is True,\n",
        "            - a numpy array of shape [batch_size] containing indicies otherwise\n",
        "\n",
        "    Preconditions:\n",
        "     - `data` is a numpy array of shape [N, 4] produced by a call\n",
        "        to `process_data`\n",
        "     - range_max > range_min\n",
        "    \"\"\"\n",
        "    xt = data[range_min:range_max, :3]\n",
        "    xt = make_onehot(xt)\n",
        "    st = data[range_min:range_max, 3]\n",
        "    if onehot:\n",
        "        st = make_onehot(st).reshape(-1, 250)\n",
        "    return xt, st\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvLuZpH-6X52"
      },
      "source": [
        "### Part (a) -- 7%\n",
        "\n",
        "We build the model in PyTorch. Since PyTorch uses automatic\n",
        "differentiation, we only need to write the *forward pass* of our\n",
        "model. \n",
        "\n",
        "**Complete** the `forward` function below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMzWMUB16X52"
      },
      "source": [
        "class PyTorchMLP(nn.Module):\n",
        "    def __init__(self, num_hidden=400):\n",
        "        super(PyTorchMLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(750, num_hidden)\n",
        "        self.layer2 = nn.Linear(num_hidden, 250)\n",
        "        self.num_hidden = num_hidden\n",
        "    def forward(self, inp):\n",
        "        inp = inp.reshape([-1, 750])\n",
        "        return self.layer2(self.layer1(inp))\n",
        "        \n",
        "        # TODO: complete this function \n",
        "        # Note that we will be using the nn.CrossEntropyLoss(), which computes the softmax operation internally, as loss criterion\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "114NF7If6X52"
      },
      "source": [
        "### Part (b) -- 10%\n",
        "\n",
        "We next  train the PyTorch model using the Adam optimizer and the cross entropy loss.\n",
        "\n",
        "**Complete** the function `run_pytorch_gradient_descent`, and use it to train your PyTorch MLP model.\n",
        "\n",
        "**Obtain** a training accuracy of at least 35% while changing only the hyperparameters of the train function.\n",
        "\n",
        "Plot the learning curve using the `plot_learning_curve` function provided\n",
        "to you, and include your plot in your PDF submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY70vUCZ6X52"
      },
      "source": [
        "def estimate_accuracy_torch(model, data, batch_size=5000, max_N=100000):\n",
        "    \"\"\"\n",
        "    Estimate the accuracy of the model on the data. To reduce\n",
        "    computation time, use at most `max_N` elements of `data` to\n",
        "    produce the estimate.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    N = 0\n",
        "    for i in range(0, data.shape[0], batch_size):\n",
        "        # get a batch of data\n",
        "        xt, st = get_batch(data, i, i + batch_size, onehot=False)\n",
        "        \n",
        "        # forward pass prediction\n",
        "        y = model(torch.Tensor(xt))\n",
        "        y = y.detach().numpy() # convert the PyTorch tensor => numpy array\n",
        "        pred = np.argmax(y, axis=1)\n",
        "        correct += np.sum(pred == st)\n",
        "        N += st.shape[0]\n",
        "\n",
        "        if N > max_N:\n",
        "            break\n",
        "    return correct / N\n",
        "\n",
        "def run_pytorch_gradient_descent(model,\n",
        "                                 train_data=train4grams,\n",
        "                                 validation_data=valid4grams,\n",
        "                                 batch_size=100,\n",
        "                                 learning_rate=0.001,\n",
        "                                 weight_decay=0,\n",
        "                                 max_iters=1000,\n",
        "                                 checkpoint_path=None):\n",
        "    \"\"\"\n",
        "    Train the PyTorch model on the dataset `train_data`, reporting\n",
        "    the validation accuracy on `validation_data`, for `max_iters`\n",
        "    iteration.\n",
        "\n",
        "    If you want to **checkpoint** your model weights (i.e. save the\n",
        "    model weights to Google Drive), then the parameter\n",
        "    `checkpoint_path` should be a string path with `{}` to be replaced\n",
        "    by the iteration count:\n",
        "\n",
        "    For example, calling \n",
        "\n",
        "    >>> run_pytorch_gradient_descent(model, ...,\n",
        "            checkpoint_path = '/content/gdrive/My Drive/Intro_to_Deep_Learning/mlp/ckpt-{}.pk')\n",
        "\n",
        "    will save the model parameters in Google Drive every 500 iterations.\n",
        "    You will have to make sure that the path exists (i.e. you'll need to create\n",
        "    the folder Intro_to_Deep_Learning, mlp, etc...). Your Google Drive will be populated with files:\n",
        "\n",
        "    - /content/gdrive/My Drive/Intro_to_Deep_Learning/mlp/ckpt-500.pk\n",
        "    - /content/gdrive/My Drive/Intro_to_Deep_Learning/mlp/ckpt-1000.pk\n",
        "    - ...\n",
        "\n",
        "    To load the weights at a later time, you can run:\n",
        "\n",
        "    >>> model.load_state_dict(torch.load('/content/gdrive/My Drive/Intro_to_Deep_Learning/mlp/ckpt-500.pk'))\n",
        "\n",
        "    This function returns the training loss, and the training/validation accuracy,\n",
        "    which we can use to plot the learning curve.\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=learning_rate,\n",
        "                           weight_decay=weight_decay)\n",
        "\n",
        "    iters, losses = [], []\n",
        "    iters_sub, train_accs, val_accs  = [], [] ,[]\n",
        "\n",
        "    n = 0 # the number of iterations\n",
        "    while True:\n",
        "        for i in range(0, train_data.shape[0], batch_size):\n",
        "            if (i + batch_size) > train_data.shape[0]:\n",
        "                break\n",
        "\n",
        "            # get the input and targets of a minibatch\n",
        "            xt, st = get_batch(train_data, i, i + batch_size, onehot=False)\n",
        "\n",
        "            # convert from numpy arrays to PyTorch tensors\n",
        "            xt = torch.Tensor(xt)\n",
        "            st = torch.Tensor(st).long()\n",
        "\n",
        "            # zs = ...                 # compute prediction logit\n",
        "            # loss =                   # compute the total loss\n",
        "            # ...                      # compute updates for each parameter\n",
        "            # ...                      # make the updates for each parameter\n",
        "            # ...                      # a clean up step for PyTorch\n",
        "            \n",
        "            zs = model.forward(xt)                 # compute prediction logit\n",
        "            loss = criterion(zs,st)                 # compute the total loss\n",
        "            loss.backward()                      # compute updates for each parameter\n",
        "            optimizer.step()                        # make the updates for each parameter\n",
        "            optimizer.zero_grad()                      # a clean up step for PyTorch\n",
        "\n",
        "\n",
        "            # save the current training information\n",
        "            iters.append(n)\n",
        "            losses.append(float(loss)/batch_size)  # compute *average* loss\n",
        "\n",
        "            if n % 500 == 0:\n",
        "                iters_sub.append(n)\n",
        "                train_cost = float(loss.detach().numpy())\n",
        "                train_acc = estimate_accuracy_torch(model, train_data)\n",
        "                train_accs.append(train_acc)\n",
        "                val_acc = estimate_accuracy_torch(model, validation_data)\n",
        "                val_accs.append(val_acc)\n",
        "                print(\"Iter %d. [Val Acc %.0f%%] [Train Acc %.0f%%, Loss %f]\" % (\n",
        "                      n, val_acc * 100, train_acc * 100, train_cost))\n",
        "\n",
        "                if (checkpoint_path is not None) and n > 0:\n",
        "                    torch.save(model.state_dict(), checkpoint_path.format(n))\n",
        "\n",
        "            # increment the iteration number\n",
        "            n += 1\n",
        "\n",
        "            if n > max_iters:\n",
        "                return iters, losses, iters_sub, train_accs, val_accs\n",
        "\n",
        "\n",
        "def plot_learning_curve(iters, losses, iters_sub, train_accs, val_accs):\n",
        "    \"\"\"\n",
        "    Plot the learning curve.\n",
        "    \"\"\"\n",
        "    plt.title(\"Learning Curve: Loss per Iteration\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Learning Curve: Accuracy per Iteration\")\n",
        "    plt.plot(iters_sub, train_accs, label=\"Train\")\n",
        "    plt.plot(iters_sub, val_accs, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXBq-1F86X52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "outputId": "1c973f01-862e-4303-beaa-f3ca4f144cc9"
      },
      "source": [
        "pytorch_mlp = PyTorchMLP()\n",
        "learning_curve_info = run_pytorch_gradient_descent(pytorch_mlp,max_iters=10000)\n",
        "plot_learning_curve(*learning_curve_info)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<generator object Module.parameters at 0x7f3edc6ca9d0>\n",
            "Iter 0. [Val Acc 1%] [Train Acc 1%, Loss 5.525949]\n",
            "Iter 500. [Val Acc 30%] [Train Acc 30%, Loss 2.891573]\n",
            "Iter 1000. [Val Acc 32%] [Train Acc 33%, Loss 2.690734]\n",
            "Iter 1500. [Val Acc 32%] [Train Acc 34%, Loss 2.673354]\n",
            "Iter 2000. [Val Acc 33%] [Train Acc 34%, Loss 2.623215]\n",
            "Iter 2500. [Val Acc 33%] [Train Acc 34%, Loss 2.686635]\n",
            "Iter 3000. [Val Acc 33%] [Train Acc 34%, Loss 2.497222]\n",
            "Iter 3500. [Val Acc 34%] [Train Acc 34%, Loss 2.479869]\n",
            "Iter 4000. [Val Acc 34%] [Train Acc 35%, Loss 2.697768]\n",
            "Iter 4500. [Val Acc 34%] [Train Acc 35%, Loss 2.706203]\n",
            "Iter 5000. [Val Acc 34%] [Train Acc 36%, Loss 2.797624]\n",
            "Iter 5500. [Val Acc 34%] [Train Acc 35%, Loss 2.513286]\n",
            "Iter 6000. [Val Acc 34%] [Train Acc 35%, Loss 2.754828]\n",
            "Iter 6500. [Val Acc 34%] [Train Acc 35%, Loss 2.645651]\n",
            "Iter 7000. [Val Acc 34%] [Train Acc 35%, Loss 3.052124]\n",
            "Iter 7500. [Val Acc 34%] [Train Acc 35%, Loss 2.668164]\n",
            "Iter 8000. [Val Acc 34%] [Train Acc 36%, Loss 2.300920]\n",
            "Iter 8500. [Val Acc 34%] [Train Acc 36%, Loss 2.930747]\n",
            "Iter 9000. [Val Acc 34%] [Train Acc 36%, Loss 2.572064]\n",
            "Iter 9500. [Val Acc 34%] [Train Acc 35%, Loss 2.668047]\n",
            "Iter 10000. [Val Acc 34%] [Train Acc 35%, Loss 2.425683]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysmNUjcY7jVq",
        "outputId": "c923f264-4f8d-4600-d0bf-cb83935e0504"
      },
      "source": [
        "for lr in [0.001,0.01,0.1,0.3]:\n",
        "  print(\"learning rate\",lr)\n",
        "  pytorch_mlp = PyTorchMLP()\n",
        "  learning_curve_info = run_pytorch_gradient_descent(pytorch_mlp,max_iters=4500)\n",
        "  print(\"====================\")\n",
        "# plot_learning_curve(*learning_curve_info)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learning rate 0.001\n",
            "<generator object Module.parameters at 0x7f3ed6b83cd0>\n",
            "Iter 0. [Val Acc 2%] [Train Acc 2%, Loss 5.511394]\n",
            "Iter 500. [Val Acc 29%] [Train Acc 30%, Loss 2.886515]\n",
            "Iter 1000. [Val Acc 32%] [Train Acc 33%, Loss 2.708653]\n",
            "Iter 1500. [Val Acc 32%] [Train Acc 34%, Loss 2.673246]\n",
            "Iter 2000. [Val Acc 33%] [Train Acc 34%, Loss 2.630921]\n",
            "Iter 2500. [Val Acc 33%] [Train Acc 34%, Loss 2.686315]\n",
            "Iter 3000. [Val Acc 33%] [Train Acc 34%, Loss 2.503071]\n",
            "Iter 3500. [Val Acc 34%] [Train Acc 34%, Loss 2.482945]\n",
            "Iter 4000. [Val Acc 34%] [Train Acc 35%, Loss 2.689580]\n",
            "Iter 4500. [Val Acc 34%] [Train Acc 35%, Loss 2.718635]\n",
            "====================\n",
            "learning rate 0.01\n",
            "<generator object Module.parameters at 0x7f3ed6b7d750>\n",
            "Iter 0. [Val Acc 1%] [Train Acc 1%, Loss 5.516646]\n",
            "Iter 500. [Val Acc 30%] [Train Acc 30%, Loss 2.891824]\n",
            "Iter 1000. [Val Acc 32%] [Train Acc 33%, Loss 2.714156]\n",
            "Iter 1500. [Val Acc 32%] [Train Acc 34%, Loss 2.667472]\n",
            "Iter 2000. [Val Acc 33%] [Train Acc 34%, Loss 2.623386]\n",
            "Iter 2500. [Val Acc 33%] [Train Acc 34%, Loss 2.687159]\n",
            "Iter 3000. [Val Acc 33%] [Train Acc 34%, Loss 2.494699]\n",
            "Iter 3500. [Val Acc 34%] [Train Acc 34%, Loss 2.486665]\n",
            "Iter 4000. [Val Acc 34%] [Train Acc 35%, Loss 2.699065]\n",
            "Iter 4500. [Val Acc 34%] [Train Acc 35%, Loss 2.718774]\n",
            "====================\n",
            "learning rate 0.1\n",
            "<generator object Module.parameters at 0x7f3ed6d52d50>\n",
            "Iter 0. [Val Acc 1%] [Train Acc 1%, Loss 5.520870]\n",
            "Iter 500. [Val Acc 30%] [Train Acc 30%, Loss 2.889787]\n",
            "Iter 1000. [Val Acc 32%] [Train Acc 33%, Loss 2.711241]\n",
            "Iter 1500. [Val Acc 32%] [Train Acc 34%, Loss 2.672279]\n",
            "Iter 2000. [Val Acc 33%] [Train Acc 34%, Loss 2.616629]\n",
            "Iter 2500. [Val Acc 33%] [Train Acc 34%, Loss 2.680289]\n",
            "Iter 3000. [Val Acc 33%] [Train Acc 34%, Loss 2.493670]\n",
            "Iter 3500. [Val Acc 34%] [Train Acc 34%, Loss 2.499091]\n",
            "Iter 4000. [Val Acc 34%] [Train Acc 35%, Loss 2.705736]\n",
            "Iter 4500. [Val Acc 33%] [Train Acc 35%, Loss 2.713581]\n",
            "====================\n",
            "learning rate 0.3\n",
            "<generator object Module.parameters at 0x7f3ed6b7d6d0>\n",
            "Iter 0. [Val Acc 4%] [Train Acc 4%, Loss 5.511839]\n",
            "Iter 500. [Val Acc 30%] [Train Acc 30%, Loss 2.897593]\n",
            "Iter 1000. [Val Acc 32%] [Train Acc 33%, Loss 2.693308]\n",
            "Iter 1500. [Val Acc 33%] [Train Acc 34%, Loss 2.675745]\n",
            "Iter 2000. [Val Acc 33%] [Train Acc 34%, Loss 2.628884]\n",
            "Iter 2500. [Val Acc 33%] [Train Acc 34%, Loss 2.694779]\n",
            "Iter 3000. [Val Acc 33%] [Train Acc 34%, Loss 2.492778]\n",
            "Iter 3500. [Val Acc 34%] [Train Acc 34%, Loss 2.487074]\n",
            "Iter 4000. [Val Acc 34%] [Train Acc 35%, Loss 2.698446]\n",
            "Iter 4500. [Val Acc 34%] [Train Acc 35%, Loss 2.711186]\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcXqpx0v6X52"
      },
      "source": [
        "### Part (c) -- 10%\n",
        "**Write** a function `make_prediction` that takes as parameters\n",
        "a PyTorchMLP model and sentence (a list of words), and produces\n",
        "a prediction for the next word in the sentence.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2jOK7B26X52"
      },
      "source": [
        "def make_prediction_torch(model, sentence):\n",
        "    \"\"\"\n",
        "    Use the model to make a prediction for the next word in the\n",
        "    sentence using the last 3 words (sentence[:-3]). You may assume\n",
        "    that len(sentence) >= 3 and that `model` is an instance of\n",
        "    PYTorchMLP.\n",
        "\n",
        "    This function should return the next word, represented as a string.\n",
        "\n",
        "    Example call:\n",
        "    >>> make_prediction_torch(pytorch_mlp, ['you', 'are', 'a'])\n",
        "    \"\"\"\n",
        "    global vocab_stoi, vocab_itos\n",
        "    xt=np.array(sentence[-3:])\n",
        "    xt = np.array([vocab_stoi[w] for w in xt])\n",
        "    xt = make_onehot(xt)\n",
        "\n",
        "    y = pytorch_mlp(torch.Tensor(xt))\n",
        "    y = y.detach().numpy() # convert the PyTorch tensor => numpy array\n",
        "    pred = np.argmax(y, axis=1)\n",
        "    return vocab_itos[pred[0]]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hse3gPHd-PLc",
        "outputId": "853f21c7-27e0-4bb8-8b9d-a66ceabea7b7"
      },
      "source": [
        "make_prediction_torch(pytorch_mlp, ['you', 'are', 'a'])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'family'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHvIKjYg6X53"
      },
      "source": [
        "### Part (d) -- 10%\n",
        "\n",
        "Use your code to predict what the next word should be in each\n",
        "of the following sentences:\n",
        "\n",
        "- \"You are a\"\n",
        "- \"few companies show\"\n",
        "- \"There are no\"\n",
        "- \"yesterday i was\"\n",
        "- \"the game had\"\n",
        "- \"yesterday the federal\"\n",
        "\n",
        "Do your predictions make sense?\n",
        "\n",
        "In many cases where you overfit the model can either output the same results for all inputs or just memorize the dataset. \n",
        "\n",
        "**Print** the output for all of these sentences and \n",
        "**Write** below if you encounter these effects or something else which indicates overfitting, if you do train again with better hyperparameters.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdzhshY56X53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4deaaed2-6b84-4aa4-8150-a93f2298205b"
      },
      "source": [
        "print(make_prediction_torch(pytorch_mlp, ['you', 'are', 'a']))\n",
        "print(make_prediction_torch(pytorch_mlp, ['few', 'companies', 'show']))\n",
        "print(make_prediction_torch(pytorch_mlp, ['there', 'are', 'no']))\n",
        "print(make_prediction_torch(pytorch_mlp, ['yesterday', 'i', 'was']))\n",
        "print(make_prediction_torch(pytorch_mlp, ['the', 'game', 'had']))\n",
        "print(make_prediction_torch(pytorch_mlp, ['yesterday', 'the', 'federal']))\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "family\n",
            ".\n",
            "one\n",
            "nt\n",
            "a\n",
            "government\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTfh4MwjAlGB"
      },
      "source": [
        "**Write your answers here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4QBM0fo6X53"
      },
      "source": [
        "### Part (e) -- 4%\n",
        "\n",
        "Report the test accuracy of your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq31oqDR6X53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "792d2413-328c-413a-d14b-9cc857628e7f"
      },
      "source": [
        "estimate_accuracy_torch(pytorch_mlp, test4grams)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.33616532721010334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlr7C8yg6X53"
      },
      "source": [
        "## Question 3. Learning Word Embeddings (20 %)\n",
        "\n",
        "In this section, we will build a slightly different model with a different\n",
        "architecture. In particular, we will first compute a lower-dimensional\n",
        "*representation* of the three words, before using a multi-layer perceptron.\n",
        "\n",
        "Our model will look like this:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=16lXygLTSuRgOCj6UWK0vHkSoyRJWfMSZ\" />\n",
        " \n",
        "\n",
        "This model has 3 layers instead of 2, but the first layer of the network\n",
        "is **not** fully-connected. Instead, we compute the representations of each\n",
        "of the three words **separately**. In addition, the first layer of the network\n",
        "will not use any biases. The reason for this will be clear in question 4.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0td55ll6X54"
      },
      "source": [
        "### Part (a) -- 8%\n",
        "\n",
        "The PyTorch model is implemented for you. Use \n",
        "`run_pytorch_gradient_descent` to train\n",
        "your PyTorch MLP model to obtain a training accuracy of at least 38%.\n",
        "Plot the learning curve using the `plot_learning_curve` function provided\n",
        "to you, and include your plot in your PDF submission.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqWlfclh6X54"
      },
      "source": [
        "class PyTorchWordEmb(nn.Module):\n",
        "    def __init__(self, emb_size=100, num_hidden=300, vocab_size=250):\n",
        "        super(PyTorchWordEmb, self).__init__()\n",
        "        self.word_emb_layer = nn.Linear(vocab_size, emb_size, bias=False)\n",
        "        self.fc_layer1 = nn.Linear(emb_size * 3, num_hidden)\n",
        "        self.fc_layer2 = nn.Linear(num_hidden, 250)\n",
        "        self.num_hidden = num_hidden\n",
        "        self.emb_size = emb_size\n",
        "    def forward(self, inp):\n",
        "        embeddings = torch.relu(self.word_emb_layer(inp))\n",
        "        embeddings = embeddings.reshape([-1, self.emb_size * 3])\n",
        "        hidden = torch.relu(self.fc_layer1(embeddings))\n",
        "        return self.fc_layer2(hidden)\n",
        "\n",
        "# pytorch_wordemb= PyTorchWordEmb()\n",
        "\n",
        "# result = run_pytorch_gradient_descent(pytorch_wordemb,\n",
        "#                                       max_iters=20000,\n",
        "#                                       ...)\n",
        "\n",
        "# plot_learning_curve(*result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oekGJRad6X54"
      },
      "source": [
        "### Part (b) -- 8%\n",
        "\n",
        "Use the function `make_prediction` that you wrote earlier to predict what the next word should be in each of the following sentences:\n",
        "\n",
        "- \"You are a\"\n",
        "- \"few companies show\"\n",
        "- \"There are no\"\n",
        "- \"yesterday i was\"\n",
        "- \"the game had\"\n",
        "- \"yesterday the federal\"\n",
        "\n",
        "How do these predictions compared to the previous model?\n",
        "\n",
        "**Print** the output for all of these sentences using the new network and \n",
        "**Write** below how the new results compare to the previous ones.\n",
        "\n",
        "Just like before, if you encounter overfitting,\n",
        "train your model for more iterations, or change the hyperparameters in your\n",
        "model. You may need to do this even if your training accuracy is >=38%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1W2Vl3g6X54"
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZQeQXPfGQNB"
      },
      "source": [
        "**Write your explanation here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g36bTOV46X54"
      },
      "source": [
        "### Part (c) -- 4%\n",
        "\n",
        "Report the test accuracy of your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy8W6XrZ6X54"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1fyrlDz6X55"
      },
      "source": [
        "## Question 4. Visualizing Word Embeddings (15%)\n",
        "\n",
        "While training the `PyTorchMLP`, we trained the `word_emb_layer`, which takes a one-hot\n",
        "representation of a word in our vocabulary, and returns a low-dimensional vector\n",
        "representation of that word. In this question, we will explore these word embeddings, which are a key concept in natural language processing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Part (a) -- 5%\n",
        "\n",
        "The code below extracts the **weights** of the word embedding layer,\n",
        "and converts the PyTorch tensor into an numpy array.\n",
        "Explain why each *row* of `word_emb` contains the vector representing\n",
        "of a word. For example `word_emb[vocab_stoi[\"any\"],:]` contains the\n",
        "vector representation of the word \"any\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IvS6JDM6X55"
      },
      "source": [
        "word_emb_weights = list(pytorch_wordemb.word_emb_layer.parameters())[0]\n",
        "word_emb = word_emb_weights.detach().numpy().T\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF_kTjxrkonT"
      },
      "source": [
        "**Write your explanation here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl-JenYz6X55"
      },
      "source": [
        "### Part (b) -- 5%\n",
        "\n",
        "One interesting thing about these word embeddings is that distances\n",
        "in these vector representations of words make some sense! To show this,\n",
        "we have provided code below that computes the *cosine similarity* of\n",
        "every pair of words in our vocabulary. This measure of similarity between vector ${\\bf v}$ and ${\\bf w}$ is defined as \n",
        "   $$d_{\\rm cos}({\\bf v},{\\bf w}) = \\frac{{\\bf v}^T{\\bf w}}{||{\\bf v}|| ||{\\bf w}||}.$$  We also pre-scale the vectors to have a unit norm, using Numpy's `norm` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPiv3pFX6X55"
      },
      "source": [
        "norms = np.linalg.norm(word_emb, axis=1)\n",
        "word_emb_norm = (word_emb.T / norms).T\n",
        "similarities = np.matmul(word_emb_norm, word_emb_norm.T)\n",
        "\n",
        "# Some example distances. The first one should be larger than the second\n",
        "print(similarities[vocab_stoi['any'], vocab_stoi['many']])\n",
        "print(similarities[vocab_stoi['any'], vocab_stoi['government']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ctM-Mgr6X55"
      },
      "source": [
        "Compute the 5 closest words to the following words:\n",
        "\n",
        "- \"four\"\n",
        "- \"go\"\n",
        "- \"what\"\n",
        "- \"should\"\n",
        "- \"school\"\n",
        "- \"your\"\n",
        "- \"yesterday\"\n",
        "- \"not\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66NCoAE26X55"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJAOG_pg6X55"
      },
      "source": [
        "### Part (c) -- 5%\n",
        "\n",
        "We can visualize the word embeddings by reducing the dimensionality of\n",
        "the word vectors to 2D. There are many dimensionality reduction techniques\n",
        "that we could use, and we will use an algorithm called t-SNE.\n",
        "(You dont need to know what this is for the assignment; we will cover it later in the course.)\n",
        "Nearby points in this 2-D space are meant to correspond to nearby points\n",
        "in the original, high-dimensional space.\n",
        "\n",
        "The following code runs the t-SNE algorithm and plots the result.\n",
        "\n",
        "Look at the plot and find at least two clusters of related words.\n",
        "\n",
        "**Write** below for each cluster what is the commonality (if there is any) and if they make sense.\n",
        "\n",
        "Note that there is randomness in the initialization of the t-SNE \n",
        "algorithm. If you re-run this code, you may get a different image.\n",
        "Please make sure to submit your image in the PDF file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seD1PgwK6X56"
      },
      "source": [
        "import sklearn.manifold\n",
        "tsne = sklearn.manifold.TSNE()\n",
        "Y = tsne.fit_transform(word_emb)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.xlim(Y[:,0].min(), Y[:, 0].max())\n",
        "plt.ylim(Y[:,1].min(), Y[:, 1].max())\n",
        "for i, w in enumerate(vocab):\n",
        "    plt.text(Y[i, 0], Y[i, 1], w)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb4gbqMam8S5"
      },
      "source": [
        "**Explain and discuss your results here:**"
      ]
    }
  ]
}